import json
import re
from docx import Document
from docx.oxml import OxmlElement
from docx.shared import Pt, RGBColor
from docx.oxml.ns import qn
from docx.enum.text import WD_PARAGRAPH_ALIGNMENT
from copy import deepcopy
from openai import OpenAI
import os
import zipfile
import difflib

try:
    import pdfplumber
except ImportError:
    pdfplumber = None


# ================== æ–‡ä»¶æ ¼å¼é¢„æ£€ (ä¿æŒä¸å˜) ==================
def validate_file_format(file_path):
    if not os.path.exists(file_path):
        return False, "æ–‡ä»¶ä¸Šä¼ å¤±è´¥ï¼Œæœªèƒ½ä¿å­˜åˆ°æœåŠ¡å™¨ã€‚"
    filename = os.path.basename(file_path)
    ext = os.path.splitext(filename)[1].lower()

    if ext == '.docx':
        if not zipfile.is_zipfile(file_path):
            return False, f"âŒ æ–‡ä»¶ã€{filename}ã€‘æ ¼å¼é”™è¯¯ï¼\nå®ƒçœ‹èµ·æ¥åƒæ˜¯æ—§ç‰ˆ .doc æˆ–å·²æŸåã€‚\nğŸ’¡ è¯·ç”¨ Word æ‰“å¼€å¹¶â€˜å¦å­˜ä¸ºâ€™ .docx æ ¼å¼ã€‚"
        try:
            Document(file_path)
        except Exception as e:
            return False, f"âŒ æ–‡ä»¶ã€{filename}ã€‘å†…å®¹æŸå: {str(e)}"
    elif ext == '.pdf':
        if pdfplumber is None:
            return False, "ç¼ºå°‘ pdfplumber åº“ã€‚"
        try:
            with pdfplumber.open(file_path) as pdf:
                if len(pdf.pages) == 0: return False, "âŒ PDF æ–‡ä»¶æ˜¯ç©ºçš„ã€‚"
        except Exception as e:
            return False, f"âŒ PDF æŸå: {str(e)}"
    return True, "OK"


# ================= æ–‡æœ¬è¯»å– (ä¿æŒä¸å˜) =================
def _read_pdf(file_path):
    if pdfplumber is None: return ""
    text_content = []
    try:
        with pdfplumber.open(file_path) as pdf:
            for i, page in enumerate(pdf.pages):
                txt = page.extract_text()
                if txt: text_content.append(f"[PDF_ç¬¬{i + 1}é¡µ] {txt}")
                tables = page.extract_tables()
                for t_idx, table in enumerate(tables):
                    clean_table = []
                    for row in table:
                        clean_row = [str(c).replace('\n', ' ') for c in row if c]
                        if clean_row: clean_table.append(" | ".join(clean_row))
                    if clean_table:
                        text_content.append(f"[PDF_è¡¨æ ¼_{i + 1}_{t_idx}]\n" + "\n".join(clean_table))
    except Exception as e:
        return f"[PDFè¯»å–å¤±è´¥] {str(e)}"
    return "\n".join(text_content)


def read_file_content(file_path):
    ext = os.path.splitext(file_path)[1].lower()
    if ext == '.pdf': return _read_pdf(file_path)
    try:
        doc = Document(file_path)
        text = []
        for i, table in enumerate(doc.tables):
            table_data = []
            for row in table.rows:
                row_txt = " | ".join([c.text.strip() for c in row.cells if c.text.strip()])
                if row_txt: table_data.append(row_txt)
            if table_data:
                text.append(f"ã€è¡¨æ ¼åŒº_{i}ã€‘\n" + "\n".join(table_data))

        para_data = []
        for p in doc.paragraphs:
            if p.text.strip(): para_data.append(p.text.strip())
        if para_data:
            text.append("ã€æ­£æ–‡åŒºã€‘\n" + "\n".join(para_data))

        return "\n\n".join(text)
    except Exception as e:
        return f"[è¯»å–é”™è¯¯] {str(e)}"


def _count_distinct_cells(row):
    elements = {id(cell._element) for cell in row.cells}
    return len(elements)


def _count_filled_cells(row):
    return sum(1 for cell in row.cells if cell.text.strip())


def _find_keyword_location(doc, keyword):
    for table in doc.tables:
        for r_idx, row in enumerate(table.rows):
            for c_idx, cell in enumerate(row.cells):
                if keyword and keyword in cell.text:
                    return table, r_idx, c_idx
    return None, -1, -1


def _find_keyword_location_fuzzy(doc, keyword, threshold=0.85):
    if not keyword:
        return None, -1, -1
    best = (None, -1, -1, 0.0)
    for table in doc.tables:
        for r_idx, row in enumerate(table.rows):
            for c_idx, cell in enumerate(row.cells):
                score = get_fuzzy_score(keyword, cell.text)
                if score > best[3]:
                    best = (table, r_idx, c_idx, score)
    if best[3] >= threshold:
        return best[0], best[1], best[2]
    return None, -1, -1


def _table_distinct_cols(table, max_rows=6):
    distinct_counts = []
    for row in table.rows[:max_rows]:
        distinct_counts.append(_count_distinct_cells(row))
    return max(distinct_counts) if distinct_counts else 0


def _row_header_hit_count(row, headers):
    if not row or not headers:
        return 0
    row_text = "".join(cell.text for cell in row.cells)
    return sum(1 for h in headers if h and h in row_text)


def _cell_vertically_merged(table, row_idx, col_idx):
    if row_idx + 1 >= len(table.rows):
        return False
    return table.rows[row_idx].cells[col_idx]._element is table.rows[row_idx + 1].cells[col_idx]._element


def normalize_plan_with_template(plan, template_path):
    """
    æ ¹æ®æ¨¡æ¿ç»“æ„ï¼Œå°†è¯¯åˆ¤çš„ lists è‡ªåŠ¨é™çº§ä¸º kvï¼Œé¿å…ç ´åæ ¼å¼ã€‚
    """
    if not plan or not plan.get("lists"):
        return plan
    if not zipfile.is_zipfile(template_path):
        return plan

    doc = Document(template_path)
    kv = plan.get("kv", [])
    lists = []

    for item in plan.get("lists", []):
        keyword = item.get("keyword", "")
        headers = item.get("headers", [])
        data = item.get("data", [])

        table, r_idx, c_idx = _find_keyword_location(doc, keyword)
        if table is None:
            table, r_idx, c_idx = _find_keyword_location_fuzzy(doc, keyword)
        if table is None:
            lists.append(item)
            continue

        row = table.rows[r_idx]
        distinct_cells = _count_distinct_cells(row)
        filled_cells = _count_filled_cells(row)

        next_row = table.rows[r_idx + 1] if r_idx + 1 < len(table.rows) else None
        next_distinct = _count_distinct_cells(next_row) if next_row else 0
        next_filled = _count_filled_cells(next_row) if next_row else 0

        header_hits = max(
            _row_header_hit_count(row, headers),
            _row_header_hit_count(next_row, headers)
        )

        data_width = 0
        for row_data in data:
            if isinstance(row_data, (list, tuple)):
                data_width = max(data_width, len(row_data))
            else:
                data_width = max(data_width, 1)

        distinct_cols = _table_distinct_cols(table)
        is_vertically_merged = _cell_vertically_merged(table, r_idx, c_idx)

        looks_like_table = (
            distinct_cols >= 3
            or next_distinct >= 3
            or (header_hits >= 2 and distinct_cols >= max(2, len(headers)))
            or (filled_cells >= 2 and next_filled >= 2 and distinct_cols >= 2)
        )

        if data_width <= 1 and distinct_cols <= 2:
            looks_like_table = False
        if is_vertically_merged and distinct_cols <= 2 and header_hits == 0:
            looks_like_table = False

        if looks_like_table:
            lists.append(item)
            continue

        row_lines = []
        for row_data in data:
            if isinstance(row_data, (list, tuple)):
                line = " ".join(str(cell).strip() for cell in row_data if str(cell).strip())
            else:
                line = str(row_data).strip()
            if line:
                row_lines.append(line)
        merged_text = "\n".join(row_lines)

        if merged_text:
            existing = next((entry for entry in kv if entry.get("anchor") == keyword), None)
            if existing:
                if existing.get("val"):
                    existing["val"] = f"{existing['val']}\n{merged_text}"
                else:
                    existing["val"] = merged_text
            else:
                kv.append({"anchor": keyword, "val": merged_text})

    plan["kv"] = kv
    plan["lists"] = lists
    return plan


def extract_docx_preview(file_path, max_paragraphs=20, max_tables=5, max_rows=8):
    if not zipfile.is_zipfile(file_path):
        return {"paragraphs": [], "tables": []}
    doc = Document(file_path)

    paragraphs = []
    for p in doc.paragraphs:
        text = p.text.strip()
        if text:
            paragraphs.append(text)
        if len(paragraphs) >= max_paragraphs:
            break

    tables = []
    for table in doc.tables[:max_tables]:
        rows = []
        for row in table.rows[:max_rows]:
            rows.append([cell.text.strip() for cell in row.cells])
        tables.append(rows)

    return {"paragraphs": paragraphs, "tables": tables}


# ================= V5 æ ¸å¿ƒ Prompt (ä¿®å¤åŸºç¡€ä¿¡æ¯é—æ¼) =================
def generate_filling_plan_v2(client, old_data, target_structure, model="deepseek-chat", temperature=0.25,
                             max_tokens=None, return_usage=False):
    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ•°æ®è¿ç§»ä¸“å®¶ã€‚

    ã€æºæ•°æ®ã€‘
    {old_data[:12000]} 

    ã€ç›®æ ‡è¡¨ç»“æ„ã€‘
    {target_structure[:4000]}

    ã€å¿…é¡»ä¸¥æ ¼æ‰§è¡Œçš„æŒ‡ä»¤ã€‘
    1. **å…¨é¢æå– KV (åŸºç¡€ä¿¡æ¯ + è½¯ä¿¡æ¯)**:
       - **åŸºç¡€ä¿¡æ¯**: å¿…é¡»åœ°æ¯¯å¼æå–æ‰€æœ‰çŸ­å­—æ®µï¼åŒ…æ‹¬â€œå­¦å·â€ã€â€œæ€§åˆ«â€ã€â€œæ°‘æ—â€ã€â€œç±è´¯â€ã€â€œæ”¿æ²»é¢è²Œâ€ã€â€œå‡ºç”Ÿå¹´æœˆâ€ç­‰ã€‚ä¸è¦å› ä¸ºå®ƒä»¬ç®€å•å°±å¿½ç•¥ï¼
       - **è½¯ä¿¡æ¯**: å¯¹äºâ€œè‡ªæˆ‘é‰´å®šâ€ã€â€œä¸»è¦äº‹è¿¹â€ç­‰é•¿æ–‡æœ¬ï¼Œå¦‚æœæºæ•°æ®æ²¡æœ‰ï¼Œè¯·**æ ¹æ®ç®€å†äº‹å®è‡ªåŠ¨æ’°å†™**ï¼Œç¦æ­¢ç•™ç©ºã€‚

    2. **Lists (å¤šè¡Œè¡¨æ ¼)**:
       - å‡¡æ˜¯ç›®æ ‡è¡¨ä¸­æœ‰æ˜ç¡®è¡¨å¤´ï¼ˆå¦‚ï¼šæ—¶é—´|è¯¾ç¨‹|æˆç»©ï¼‰çš„ï¼Œå¿…é¡»æå–ä¸º `lists`ã€‚
       - **ä¸¥æ ¼å¯¹é½**: `headers` åˆ—æ•°å¿…é¡»ä¸ `data` åˆ—æ•°ä¸€è‡´ã€‚

    3. **Checkbox (å‹¾é€‰æ¡†)**:
       - å¯»æ‰¾â€œâ–¡â€ç¬¦å·ã€‚
       - è¾“å‡º keyword (é€‰é¡¹æ–‡å­—) å’Œ status (æœ‰/æ— /æ˜¯/å¦)ã€‚

    ã€è¾“å‡ºæ ¼å¼ (JSON)ã€‘
    {{
        "kv": [
            {{"anchor": "å§“å", "val": "å¼ ä¸‰"}},
            {{"anchor": "å­¦å·", "val": "20201101"}},
            {{"anchor": "æ€§åˆ«", "val": "ç”·"}},
            {{"anchor": "è‡ªæˆ‘é‰´å®š", "val": "æœ¬äººåœ¨æ ¡æœŸé—´..."}}
        ],
        "checkbox": [
            {{"keyword": "å…šå‘˜", "status": "æœ‰"}},
            {{"keyword": "è‹±è¯­å…­çº§", "status": "æ— "}}
        ],
        "lists": [
            {{
                "keyword": "è·å¥–æƒ…å†µ", 
                "headers": ["æ—¶é—´", "å¥–é¡¹", "ç­‰çº§"],
                "data": [["2023.09", "ä¸€ç­‰å¥–", "æ ¡çº§"]]
            }}
        ]
    }}
    """
    request_kwargs = {
        "model": model,
        "messages": [{"role": "user", "content": prompt}],
        "temperature": temperature
    }
    if max_tokens is not None:
        request_kwargs["max_tokens"] = max_tokens
    response = client.chat.completions.create(**request_kwargs)
    content = response.choices[0].message.content
    content = re.sub(r'```json\s*|\s*```', '', content)

    try:
        plan = json.loads(content)

        # è‡ªåŠ¨æ¸…æ´—è¡¨æ ¼åˆ—æ•° (é˜²æ­¢æŠ¥é”™)
        if "lists" in plan:
            for lst in plan["lists"]:
                headers = lst.get("headers", [])
                data = lst.get("data", [])
                if headers and data:
                    num_cols = len(headers)
                    cleaned_data = []
                    for row in data:
                        if len(row) > num_cols:
                            cleaned_data.append(row[:num_cols])
                        elif len(row) < num_cols:
                            cleaned_data.append(row + [""] * (num_cols - len(row)))
                        else:
                            cleaned_data.append(row)
                    lst["data"] = cleaned_data

        if return_usage:
            return plan, getattr(response, "usage", None)
        return plan
    except:
        if return_usage:
            return {"kv": [], "checkbox": [], "lists": []}, getattr(response, "usage", None)
        return {"kv": [], "checkbox": [], "lists": []}


def refine_text_v2(client, original_text, instruction, model="deepseek-chat", temperature=0.7, max_tokens=None,
                   return_usage=False):
    prompt = f"åŸæ–‡ï¼š{original_text}\næŒ‡ä»¤ï¼š{instruction}\nè¯·è¾“å‡ºä¿®æ”¹åçš„ç»“æœï¼š"
    request_kwargs = {
        "model": model,
        "messages": [{"role": "user", "content": prompt}],
        "temperature": temperature
    }
    if max_tokens is not None:
        request_kwargs["max_tokens"] = max_tokens
    response = client.chat.completions.create(**request_kwargs)
    content = response.choices[0].message.content
    if return_usage:
        return content, getattr(response, "usage", None)
    return content


# ================= å†™å…¥é€»è¾‘ =================

def force_write_cell(cell, text, alignment="auto"):
    """
    æ ¼å¼ç¾åŒ–ï¼šæ¸…é™¤åŸæœ‰æ ¼å¼ï¼Œè‡ªåŠ¨åˆ¤æ–­å±…ä¸­æˆ–å·¦å¯¹é½
    """
    # ä¿æŠ¤æ€§æ£€æŸ¥ï¼šå¦‚æœ text æ˜¯ Noneï¼Œè½¬ä¸ºç©ºå­—ç¬¦ä¸²
    if text is None: text = ""

    cell._element.clear_content()
    p = cell.add_paragraph()

    text_len = len(str(text))
    if alignment == "auto":
        # çŸ­æ–‡æœ¬å±…ä¸­ï¼Œé•¿æ–‡æœ¬å·¦å¯¹é½
        if text_len < 15 and "\n" not in str(text):
            p.alignment = WD_PARAGRAPH_ALIGNMENT.CENTER
        else:
            p.alignment = WD_PARAGRAPH_ALIGNMENT.LEFT

    run = p.add_run(str(text))
    run.font.name = 'å®‹ä½“'
    run._element.rPr.rFonts.set(qn('w:eastAsia'), 'å®‹ä½“')
    run.font.size = Pt(10.5)
    run.font.color.rgb = RGBColor(0, 0, 0)


def append_write_cell(cell, text, alignment="left"):
    """
    ä¿ç•™åŸæœ‰å†…å®¹ï¼Œè¿½åŠ æ–°å†…å®¹ï¼ˆé¿å…è¦†ç›–æ ‡é¢˜/æ ‡ç­¾ï¼‰
    """
    if text is None:
        text = ""
    if not cell.text.strip():
        force_write_cell(cell, text, alignment=alignment)
        return

    p = cell.add_paragraph()
    if alignment == "left":
        p.alignment = WD_PARAGRAPH_ALIGNMENT.LEFT
    run = p.add_run(str(text))
    run.font.name = 'å®‹ä½“'
    run._element.rPr.rFonts.set(qn('w:eastAsia'), 'å®‹ä½“')
    run.font.size = Pt(10.5)
    run.font.color.rgb = RGBColor(0, 0, 0)


def get_next_distinct_cell(row, current_idx):
    current_cell = row.cells[current_idx]
    for i in range(current_idx + 1, len(row.cells)):
        next_cell = row.cells[i]
        if next_cell._element is not current_cell._element:
            return next_cell
    return None


def find_next_writable_cell(row, current_idx):
    current_cell = row.cells[current_idx]
    for i in range(current_idx + 1, len(row.cells)):
        next_cell = row.cells[i]
        if next_cell._element is current_cell._element:
            continue
        if not next_cell.text.strip():
            return next_cell
    return None


def handle_checkbox(cell, status):
    text = cell.text
    new_text = text

    # ====== ã€å…³é”®ä¿®å¤ã€‘: ä½¿ç”¨ç©·ä¸¾åŒ¹é…æ³•ï¼Œè§£å†³â€œæœ‰â–¡ æ— â–¡â€åŒæ ¼é—®é¢˜ ======

    # 1. æ˜ å°„è¡¨ï¼šçŠ¶æ€ -> éœ€è¦æ›¿æ¢çš„ç›®æ ‡å­—ç¬¦ä¸²
    replace_map = {}

    if status in ["æ— ", "No", "å¦", "None", "æœªé€šè¿‡"]:
        replace_map = {
            "æ— â–¡": "æ— â˜‘", "æ—  â–¡": "æ—  â˜‘",
            "å¦â–¡": "å¦â˜‘", "å¦ â–¡": "å¦ â˜‘",
            "æœªé€šè¿‡â–¡": "æœªé€šè¿‡â˜‘"
        }
    elif status in ["æœ‰", "Yes", "æ˜¯", "Have", "é€šè¿‡"]:
        replace_map = {
            "æœ‰â–¡": "æœ‰â˜‘", "æœ‰ â–¡": "æœ‰ â˜‘",
            "æ˜¯â–¡": "æ˜¯â˜‘", "æ˜¯ â–¡": "æ˜¯ â˜‘",
            "é€šè¿‡â–¡": "é€šè¿‡â˜‘"
        }

    # 2. ä¼˜å…ˆå°è¯•åŒ…å«æ–‡å­—çš„ç²¾ç¡®æ›¿æ¢ (è§£å†³ "æœ‰â–¡ æ— â–¡" è¿™ç§åœºæ™¯)
    replaced_flag = False
    for k, v in replace_map.items():
        if k in text:
            new_text = new_text.replace(k, v)
            replaced_flag = True

    # 3. å¦‚æœæ²¡æ‰¾åˆ°å¸¦æ–‡å­—çš„æ¡†ï¼Œä½†ç¡®å®æ˜¯å‹¾é€‰çŠ¶æ€ï¼Œä¸”æ ¼å­é‡Œåªæœ‰ä¸€ä¸ªæ¡†ï¼Œåˆ™å…œåº•æ›¿æ¢
    if not replaced_flag and "â–¡" in text:
        # åªæœ‰å½“çŠ¶æ€æ˜ç¡®ä¸ºè‚¯å®šï¼Œæˆ–è€…æ˜ç¡®é’ˆå¯¹è¯¥é¡¹æ—¶æ‰æ‰“é’©
        if status in ["æœ‰", "Yes", "æ˜¯", "Have", "True"]:
            new_text = text.replace("â–¡", "â˜‘", 1)  # åªæ›¿ç¬¬ä¸€ä¸ªï¼Œé˜²æ­¢è¯¯ä¼¤

    if new_text != text:
        cell.text = new_text
        return True
    return False


def deepcopy_row(table, source_row):
    tbl = table._tbl
    tr = deepcopy(source_row._tr)
    source_row._tr.addprevious(tr)
    return table.rows[source_row._index - 1]


def get_fuzzy_score(anchor, target_text):
    a = anchor.replace(" ", "").replace("\n", "").lower()
    t = target_text.replace(" ", "").replace("\n", "").lower()
    if not a or not t: return 0.0
    if a == t: return 1.0
    if a in t: return 1.0
    return difflib.SequenceMatcher(None, a, t).ratio()


def is_placeholder_text(text):
    clean = text.replace(" ", "")
    return any(k in clean for k in ["æ­¤æ ", "å¡«å†™", "è¯´æ˜", "ç®€è¿°", "å¤‡æ³¨"])


# --- è¾…åŠ©å‡½æ•°ï¼šè®¾ç½®å•å…ƒæ ¼ä¸ºçºµå‘åˆå¹¶çš„â€œç»§ç»­â€çŠ¶æ€ ---
def set_cell_merge_continue(cell):
    """
    ä¿®æ”¹å•å…ƒæ ¼ XMLï¼Œä½¿å…¶å±æ€§å˜ä¸º vMerge="continue" (å³åˆå¹¶å•å…ƒæ ¼çš„éé¦–è¡Œéƒ¨åˆ†)
    """
    tc = cell._tc
    tcPr = tc.get_or_add_tcPr()
    # æŸ¥æ‰¾ç°æœ‰çš„ vMerge
    vMerge = tcPr.find(qn('w:vMerge'))
    if vMerge is None:
        vMerge = OxmlElement('w:vMerge')
        tcPr.append(vMerge)
    # vMerge æ ‡ç­¾æ²¡æœ‰ val å±æ€§æ—¶ï¼Œé»˜è®¤ä¸º continue
    if 'w:val' in vMerge.attrib:
        del vMerge.attrib['w:val']

# --- è¾…åŠ©å‡½æ•°ï¼šåœ¨æŒ‡å®šè¡Œä¹‹åæ’å…¥æ–°è¡Œ ---
def insert_row_after(table, ref_row):
    """
    åœ¨ ref_row ä¹‹åæ’å…¥ä¸€è¡Œï¼Œå¹¶å¤åˆ¶ ref_row çš„æ ·å¼
    """
    tbl = table._tbl
    new_tr = deepcopy(ref_row._tr)
    ref_row._tr.addnext(new_tr)
    # æ‰¾åˆ°æ–°æ’å…¥çš„è¡Œå¯¹è±¡
    new_row_idx = ref_row._index + 1
    return table.rows[new_row_idx]

def get_row_merge_range(table, row_idx, col_idx):
    """
    è®¡ç®—çºµå‘åˆå¹¶èŒƒå›´ (start, end)
    """
    start_cell = table.rows[row_idx].cells[col_idx]
    start_element = start_cell._element
    end_row = row_idx
    for r in range(row_idx + 1, len(table.rows)):
        current_cell = table.rows[r].cells[col_idx]
        if current_cell._element is start_element:
            end_row = r
        else:
            break
    return row_idx, end_row

def find_column_index_by_header(row, header_texts):
    mapping = {}
    for idx, cell in enumerate(row.cells):
        txt = cell.text.strip().replace(" ", "")
        for h in header_texts:
            if h in txt:
                mapping[h] = idx
    return mapping

def execute_word_writing_v2(plan, template_path, output_path, progress_callback=None):
    if not zipfile.is_zipfile(template_path):
        raise ValueError("ç›®æ ‡æ–‡ä»¶æ ¼å¼é”™è¯¯")
    doc = Document(template_path)

    # ---------------- 1. KV å†™å…¥ ----------------
    total_kv = len(plan.get("kv", []))
    anchor_usage = {}
    for i, item in enumerate(plan.get("kv", [])):
        anchor, val = item["anchor"], item["val"]
        if not val: continue

        if progress_callback: progress_callback(int(10 + (i / total_kv) * 30), f"æ­£åœ¨å†™å…¥: {anchor}...")

        found = False
        for table in doc.tables:
            for row in table.rows:
                for c_idx, cell in enumerate(row.cells):
                    cell_text = cell.text.strip().replace(" ", "")
                    clean_anchor = anchor.strip().replace(" ", "")
                    match_score = get_fuzzy_score(clean_anchor, cell_text)
                    strict_match = clean_anchor in cell_text or cell_text in clean_anchor
                    short_anchor = len(clean_anchor) <= 8
                    if (short_anchor and not strict_match):
                        continue

                    if strict_match or match_score > 0.9:
                        used_cells = anchor_usage.get(anchor, set())
                        if cell._element in used_cells:
                            continue
                        target_cell = None
                        candidate = find_next_writable_cell(row, c_idx)

                        # ä¼˜å…ˆå†™å…¥ç›¸é‚»å¯å†™å•å…ƒæ ¼
                        if candidate:
                            target_cell = candidate
                        else:
                            # æ— ç›¸é‚»å•å…ƒæ ¼æ—¶ï¼Œæ‰è€ƒè™‘å†™å…¥å½“å‰å•å…ƒæ ¼
                            target_cell = cell

                        if target_cell:
                            # ä¿æŠ¤æœºåˆ¶ï¼šé˜²æ­¢è¦†ç›–è¡¨å¤´
                            # å¦‚æœç›®æ ‡æ ¼å­å¾ˆçŸ­ï¼Œä¸”åŒ…å«å†’å·æˆ–çœ‹èµ·æ¥åƒå¦ä¸€ä¸ªè¡¨å¤´ï¼Œè·³è¿‡
                            if len(target_cell.text) < 10 and ("ï¼š" in target_cell.text or ":" in target_cell.text):
                                pass
                            else:
                                if target_cell is cell:
                                    append_write_cell(target_cell, val, alignment="left")
                                else:
                                    force_write_cell(target_cell, val, alignment="auto")
                                used_cells.add(cell._element)
                                anchor_usage[anchor] = used_cells
                                found = True
                                break
                if found: break
            if found: break

    # ---------------- 2. Checkbox å†™å…¥ (æ–°ç‰ˆåŒ¹é…é€»è¾‘) ----------------
    if progress_callback: progress_callback(60, "å¤„ç†å‹¾é€‰æ¡†...")
    for item in plan.get("checkbox", []):
        keyword, status = item["keyword"], item["status"]
        for table in doc.tables:
            for row in table.rows:
                for cell in row.cells:
                    # åªæœ‰å½“å…³é”®å­—åŒ¹é…æ—¶æ‰å°è¯•æ‰“é’©
                    if keyword in cell.text:
                        handle_checkbox(cell, status)

    # 3. Lists å†™å…¥ (âœ¨ ä¿®å¤è¡¨å¤´è¢«é¡¶é£çš„é—®é¢˜ âœ¨)
    if progress_callback: progress_callback(80, "å¤„ç†è¡¨æ ¼åˆ—è¡¨...")

    for item in plan.get("lists", []):
        keyword = item["keyword"]
        headers = item.get("headers", [])
        data = item.get("data", [])

        if not data: continue

        # A. å®šä½é”šç‚¹
        target_table = None
        anchor_row_idx = -1
        anchor_col_idx = -1

        found = False
        for t_idx, table in enumerate(doc.tables):
            for r_idx, row in enumerate(table.rows):
                for c_idx, cell in enumerate(row.cells):
                    if keyword in cell.text:
                        target_table = table
                        anchor_row_idx = r_idx
                        anchor_col_idx = c_idx
                        found = True
                        break
                if found: break
            if found: break

        if not found:
            continue

        # B. åˆ¤è¯»å½“å‰ç‰ˆå—ç±»å‹
        start_r, end_r = get_row_merge_range(target_table, anchor_row_idx, anchor_col_idx)
        is_side_block = (end_r > start_r)  # æ˜¯å¦ä¸ºä¾§è¾¹æ åˆå¹¶ç±»å‹

        # C. æ™ºèƒ½ç¡®å®šæ•°æ®èµ·å§‹è¡Œ (Fix: é˜²æ­¢å†™åœ¨è¡¨å¤´ä¸Šé¢)
        header_map = find_column_index_by_header(target_table.rows[anchor_row_idx], headers)
        data_start_row = anchor_row_idx  # é»˜è®¤ä»é”šç‚¹è¡Œå¼€å§‹ç®—

        # ç­–ç•¥ï¼šå‘ä¸‹ä¸€è¡Œæ¢æµ‹
        if anchor_row_idx + 1 < len(target_table.rows):
            next_row = target_table.rows[anchor_row_idx + 1]
            next_row_text = "".join([c.text for c in next_row.cells]).strip()

            # 1. å°è¯•åœ¨ä¸‹ä¸€è¡Œç²¾å‡†åŒ¹é…è¡¨å¤´
            candidate_map = find_column_index_by_header(next_row, headers)

            if candidate_map:
                # å‘½ä¸­è¡¨å¤´ï¼
                header_map = candidate_map
                data_start_row = anchor_row_idx + 1

            elif not is_side_block:
                # 2. ã€å…³é”®ä¿®å¤ã€‘å¦‚æœæ˜¯æ™®é€šæ¨¡å¼ï¼Œä¸”æ²¡åŒ¹é…åˆ°è¡¨å¤´ï¼Œä½†ä¸‹ä¸€è¡Œæ˜æ˜¾æœ‰æ–‡å­—
                # æˆ‘ä»¬å‡è®¾ä¸‹ä¸€è¡Œå°±æ˜¯è¡¨å¤´ï¼ˆæ¯”å¦‚ Word é‡Œæ˜¯â€œè¯¾ç¨‹åç§°â€ï¼ŒAI è¯†åˆ«æˆâ€œè¯¾ç¨‹åâ€ï¼Œå¯¼è‡´æ²¡åŒ¹é…ä¸Šï¼‰
                # è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¼ºåˆ¶è·³è¿‡ä¸‹ä¸€è¡Œï¼Œä»ä¸‹ä¸‹è¡Œå¼€å§‹å†™
                if len(next_row_text) > 2 and (
                        "è¯¾ç¨‹" in next_row_text or "åç§°" in next_row_text or "æˆç»©" in next_row_text or "Date" in next_row_text):
                    data_start_row = anchor_row_idx + 1  # æŠŠä¸‹ä¸€è¡Œè§†ä¸ºè¡¨å¤´

        # ç¡®å®šæ¸¸æ ‡åˆå§‹ä½ç½®ï¼šæ€»æ˜¯ä» data_start_row çš„ä¸‹ä¸€è¡Œå¼€å§‹å†™
        cursor_row_idx = data_start_row + 1

        # ã€å…³é”®ä¿®å¤ã€‘åŒæ­¥è¾¹ç•Œ end_r
        # å¦‚æœæ˜¯æ™®é€šæ¨¡å¼ï¼Œend_r åˆå§‹å€¼å¯èƒ½å°±æ˜¯é”šç‚¹è¡Œ(0)ã€‚
        # ä½†å¦‚æœæˆ‘ä»¬è¦ä» Row 2 å¼€å§‹å†™ï¼Œå¿…é¡»æŠŠ end_r è‡³å°‘æ¨åˆ° Row 2 çš„å‰ä¸€è¡Œï¼Œé˜²æ­¢ cursor > end_r å¯¼è‡´åœ¨ Row 0 åé¢æ’å…¥
        # ç®€å•æ¥è¯´ï¼šåœ¨æ™®é€šæ¨¡å¼ä¸‹ï¼Œåªè¦è¡¨æ ¼é‡Œè¿˜æœ‰ç©ºè¡Œï¼Œå°±ä¸è¦æ€¥ç€æ’å…¥ã€‚
        if not is_side_block:
            # åªè¦ cursor æŒ‡å‘çš„è¡Œå­˜åœ¨ï¼Œæˆ‘ä»¬å°±è®¤ä¸ºå®ƒåœ¨è¾¹ç•Œå†…
            if cursor_row_idx < len(target_table.rows):
                end_r = max(end_r, cursor_row_idx)

        # D. å¾ªç¯å¡«å…¥æ•°æ®
        for data_idx, data_row in enumerate(data):

            # æ£€æŸ¥æ˜¯å¦è¶Šç•Œ/éœ€è¦æ‰©å®¹
            if cursor_row_idx > end_r:

                # === æ‰©å®¹é€»è¾‘ ===
                # å¤åˆ¶æ¨¡æ¿ï¼šæ™®é€šæ¨¡å¼ä¸‹ï¼Œå°½é‡å¤åˆ¶ä¸Šä¸€è¡Œï¼ˆå³æ•°æ®è¡Œæ ·å¼ï¼‰ï¼›ä¾§è¾¹æ æ¨¡å¼å¤åˆ¶æœ€åä¸€è¡Œ
                template_row_idx = end_r
                if not is_side_block and cursor_row_idx > 0:
                    template_row_idx = cursor_row_idx - 1

                # å®‰å…¨æ£€æŸ¥
                if template_row_idx >= len(target_table.rows): template_row_idx = len(target_table.rows) - 1

                last_row = target_table.rows[template_row_idx]
                new_row = insert_row_after(target_table, last_row)

                # === æ ·å¼å¤„ç† ===
                if is_side_block:
                    # ä¾§è¾¹æ æ¨¡å¼ï¼šä¿ç•™é”šç‚¹åˆ—ï¼Œæ¸…ç©ºå…¶ä»–
                    for idx, cell in enumerate(new_row.cells):
                        if idx == anchor_col_idx:
                            pass
                        else:
                            cell._element.clear_content()
                    # ä¿®å¤å·¦ä¾§åˆå¹¶
                    merge_cell = new_row.cells[anchor_col_idx]
                    set_cell_merge_continue(merge_cell)
                else:
                    # æ™®é€šæ¨¡å¼ï¼šæ¸…ç©ºæ‰€æœ‰åˆ—ï¼Œä¸åˆå¹¶
                    for cell in new_row.cells:
                        cell._element.clear_content()

                end_r += 1
                # ===================

            # E. æ‰§è¡Œå†™å…¥
            if cursor_row_idx >= len(target_table.rows): break
            current_row = target_table.rows[cursor_row_idx]

            if header_map:
                # æœ‰è¡¨å¤´æ˜ å°„
                for h_text, col_idx in header_map.items():
                    try:
                        val_idx = headers.index(h_text)
                        if val_idx < len(data_row):
                            force_write_cell(current_row.cells[col_idx], data_row[val_idx])
                    except:
                        pass
            else:
                # æ— è¡¨å¤´æ˜ å°„ï¼ˆç›²å¡«ï¼‰
                start_col = anchor_col_idx + 1 if is_side_block else 0
                write_col = start_col
                data_ptr = 0
                while write_col < len(current_row.cells) and data_ptr < len(data_row):
                    cell = current_row.cells[write_col]
                    # è·³è¿‡åˆå¹¶åˆ—(æ°´å¹³)
                    if write_col > 0 and cell._element is current_row.cells[write_col - 1]._element:
                        write_col += 1
                        continue
                    force_write_cell(cell, data_row[data_ptr])
                    data_ptr += 1
                    write_col += 1

            cursor_row_idx += 1

    doc.save(output_path)
    if progress_callback: progress_callback(100, "å®Œæˆ")
